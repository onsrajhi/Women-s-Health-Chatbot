# -*- coding: utf-8 -*-
"""DialoGPT Fine.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1uEQvUjfPMPRTzrRCLVj1u1jAz04X0oga
"""

!pip install transformers datasets accelerate peft gradio

import json
from datasets import Dataset

with open("merged_data.json", "r") as f:
    qa_data = json.load(f)

dialogues = []
for pair in qa_data:
    text = f"User: {pair['question']}\nBot: {pair['answer']}"
    dialogues.append({"text": text})

dataset = Dataset.from_list(dialogues)
print(dataset)

from transformers import AutoModelForCausalLM, AutoTokenizer

model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)

tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(model_name)

from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["c_attn", "q_attn"],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)

model = get_peft_model(model, lora_config)
model.print_trainable_parameters()

def tokenize_fn(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)

tokenized_dataset = dataset.map(tokenize_fn, batched=True)

def preprocess_function(examples):
    inputs = tokenizer(examples["text"], padding=True, truncation=True, return_tensors="pt")
    inputs["labels"] = inputs["input_ids"].clone()
    return inputs

tokenized_dataset = dataset.map(preprocess_function, batched=True)

print(tokenized_dataset[25])

from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./lora-chatbot",
    per_device_train_batch_size=2,
    num_train_epochs=3,
    learning_rate=2e-4,
    logging_steps=10,
    save_steps=500,
    save_total_limit=2,
    fp16=True,
    label_names=["labels"] ,
     report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset
)

trainer.save_model("./lora-chatbot")

import os
os.environ["WANDB_DISABLED"] = "true"

trainer.train()

from peft import PeftModel
from transformers import AutoModelForCausalLM

base_model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-small")
model = PeftModel.from_pretrained(base_model, "./lora-chatbot")

from peft import PeftModel
import torch


base_model = AutoModelForCausalLM.from_pretrained(model_name)
lora_model = PeftModel.from_pretrained(base_model, "./lora-chatbot")

device = "cuda" if torch.cuda.is_available() else "cpu"
lora_model = lora_model.to(device)

def chat_with_bot(user_input, chat_history_ids=None):
    new_input_ids = tokenizer.encode(user_input + tokenizer.eos_token, return_tensors="pt").to(device)

    bot_input_ids = (
        torch.cat([chat_history_ids, new_input_ids], dim=-1)
        if chat_history_ids is not None
        else new_input_ids
    )

    chat_history_ids = lora_model.generate(
        bot_input_ids,
        max_length=1000,
        pad_token_id=tokenizer.eos_token_id
    )

    bot_output = tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)
    return bot_output, chat_history_ids


chat_history_ids = None
while True:
    user_input = input("You:")
    if user_input.lower() == "quit":
        break
    bot_output, chat_history_ids = chat_with_bot("User: " + user_input, chat_history_ids)
    print("Bot:", bot_output)

def chat_response(user_input):
    inputs = tokenizer(user_input, return_tensors="pt", padding=True, truncation=True)
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    output_ids = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_length=50,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_p=0.9,
        top_k=50
    )

    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return response

print("User: Hi")
print("Bot:", chat_response("Hi"))

print("User: How are you?")
print("Bot:", chat_response("How are you?"))

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch


model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)


tokenizer.pad_token = tokenizer.eos_token

def chat_response(user_input):
    inputs = tokenizer(user_input, return_tensors="pt", padding=True, truncation=True)
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    output_ids = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_length=50,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_p=0.9,
        top_k=50
    )

    response = tokenizer.decode(output_ids[0], skip_special_tokens=True)
    return response


print("User: Hi")
print("Bot:", chat_response("Hi"))

print("User: How are you?")
print("Bot:", chat_response("How are you?"))

!pip install transformers gradio

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import gradio as gr

model_name = "microsoft/DialoGPT-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)
tokenizer.pad_token = tokenizer.eos_token

def generate_responses(user_input):
    inputs = tokenizer(user_input, return_tensors="pt", padding=True, truncation=True)
    input_ids = inputs["input_ids"]
    attention_mask = inputs["attention_mask"]

    output_ids = model.generate(
        input_ids=input_ids,
        attention_mask=attention_mask,
        max_length=100,
        pad_token_id=tokenizer.eos_token_id,
        do_sample=True,
        top_p=0.8,
        top_k=100,
        temperature=0.7,
        num_return_sequences=5
    )

    responses = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]
    return "\n\n".join([f"ðŸ‘‰ {r}" for r in responses])

iface = gr.Interface(
    fn=generate_responses,
    inputs=gr.Textbox(lines=2, placeholder="Type here..."),
    outputs="text",
    title="Chatbot DialoGPT with Varied Responses",
    description="It gives you 4 different response suggestions, try it!")

iface.launch()